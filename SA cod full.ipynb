{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d71c00e-ffca-45fe-8127-3742c8367fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1220871-fe81-4da8-8e69-b3969510c39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('2013.txt')\n",
    "# Define a function to clean the text\n",
    "def clean(text):\n",
    "# Removes all special characters and numericals leaving the alphabets\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Cleaning the text in the review column\n",
    "mydata['Cleaned Data'] = mydata['article_content'].apply(clean)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist\n",
    "\n",
    "mydata['POS tagged'] = mydata['Cleaned Data'].apply(token_stop_pos)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew\n",
    "\n",
    "mydata['Lemma'] = mydata['POS tagged'].apply(lemmatize)\n",
    "from textblob import TextBlob\n",
    "# function to calculate subjectivity\n",
    "def getSubjectivity(review):\n",
    "    return TextBlob(review).sentiment.subjectivity\n",
    "    # function to calculate polarity\n",
    "def getPolarity(review):\n",
    "        return TextBlob(review).sentiment.polarity\n",
    "\n",
    "# function to analyze the reviews\n",
    "def analysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "fin_data = pd.DataFrame(mydata[['date', 'article_content', 'Lemma']])\n",
    "fin_data['Subjectivity'] = fin_data['Lemma'].apply(getSubjectivity) \n",
    "fin_data['Polarity'] = fin_data['Lemma'].apply(getPolarity) \n",
    "fin_data['Analysis'] = fin_data['Polarity'].apply(analysis)\n",
    "tb_counts = fin_data.Analysis.value_counts()\n",
    "i = fin_data[(fin_data.Analysis == 'Neutral')].index\n",
    "sentiment = fin_data.drop(i)\n",
    "sentiment\n",
    "sentiment.set_index('date')\n",
    "sentiment.to_csv('sentiment2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1925deb7-478e-4018-ad0c-d333de528c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('2014.txt')\n",
    "# Define a function to clean the text\n",
    "def clean(text):\n",
    "# Removes all special characters and numericals leaving the alphabets\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Cleaning the text in the review column\n",
    "mydata['Cleaned Data'] = mydata['article_content'].apply(clean)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist\n",
    "\n",
    "mydata['POS tagged'] = mydata['Cleaned Data'].apply(token_stop_pos)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew\n",
    "\n",
    "mydata['Lemma'] = mydata['POS tagged'].apply(lemmatize)\n",
    "from textblob import TextBlob\n",
    "# function to calculate subjectivity\n",
    "def getSubjectivity(review):\n",
    "    return TextBlob(review).sentiment.subjectivity\n",
    "    # function to calculate polarity\n",
    "def getPolarity(review):\n",
    "        return TextBlob(review).sentiment.polarity\n",
    "\n",
    "# function to analyze the reviews\n",
    "def analysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "fin_data = pd.DataFrame(mydata[['date', 'article_content', 'Lemma']])\n",
    "fin_data['Subjectivity'] = fin_data['Lemma'].apply(getSubjectivity) \n",
    "fin_data['Polarity'] = fin_data['Lemma'].apply(getPolarity) \n",
    "fin_data['Analysis'] = fin_data['Polarity'].apply(analysis)\n",
    "tb_counts = fin_data.Analysis.value_counts()\n",
    "i = fin_data[(fin_data.Analysis == 'Neutral')].index\n",
    "sentiment = fin_data.drop(i)\n",
    "sentiment\n",
    "sentiment.set_index('date')\n",
    "sentiment.to_csv('sentiment3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d249872e-8dee-488b-9e00-d8327fae8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('2015.txt')\n",
    "# Define a function to clean the text\n",
    "def clean(text):\n",
    "# Removes all special characters and numericals leaving the alphabets\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Cleaning the text in the review column\n",
    "mydata['Cleaned Data'] = mydata['article_content'].apply(clean)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist\n",
    "\n",
    "mydata['POS tagged'] = mydata['Cleaned Data'].apply(token_stop_pos)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew\n",
    "\n",
    "mydata['Lemma'] = mydata['POS tagged'].apply(lemmatize)\n",
    "from textblob import TextBlob\n",
    "# function to calculate subjectivity\n",
    "def getSubjectivity(review):\n",
    "    return TextBlob(review).sentiment.subjectivity\n",
    "    # function to calculate polarity\n",
    "def getPolarity(review):\n",
    "        return TextBlob(review).sentiment.polarity\n",
    "\n",
    "# function to analyze the reviews\n",
    "def analysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "fin_data = pd.DataFrame(mydata[['date', 'article_content', 'Lemma']])\n",
    "fin_data['Subjectivity'] = fin_data['Lemma'].apply(getSubjectivity) \n",
    "fin_data['Polarity'] = fin_data['Lemma'].apply(getPolarity) \n",
    "fin_data['Analysis'] = fin_data['Polarity'].apply(analysis)\n",
    "tb_counts = fin_data.Analysis.value_counts()\n",
    "i = fin_data[(fin_data.Analysis == 'Neutral')].index\n",
    "sentiment = fin_data.drop(i)\n",
    "sentiment\n",
    "sentiment.set_index('date')\n",
    "sentiment.to_csv('sentiment4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9213effb-7fe9-4bc2-90ed-a57e4d3a6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('2016.txt')\n",
    "# Define a function to clean the text\n",
    "def clean(text):\n",
    "# Removes all special characters and numericals leaving the alphabets\n",
    "    text = re.sub('[^A-Za-z]+', ' ', str(text))\n",
    "    return text\n",
    "\n",
    "# Cleaning the text in the review column\n",
    "mydata['Cleaned Data'] = mydata['article_content'].apply(clean)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist\n",
    "\n",
    "mydata['POS tagged'] = mydata['Cleaned Data'].apply(token_stop_pos)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew\n",
    "\n",
    "mydata['Lemma'] = mydata['POS tagged'].apply(lemmatize)\n",
    "from textblob import TextBlob\n",
    "# function to calculate subjectivity\n",
    "def getSubjectivity(review):\n",
    "    return TextBlob(review).sentiment.subjectivity\n",
    "    # function to calculate polarity\n",
    "def getPolarity(review):\n",
    "        return TextBlob(review).sentiment.polarity\n",
    "\n",
    "# function to analyze the reviews\n",
    "def analysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "fin_data = pd.DataFrame(mydata[['date', 'article_content', 'Lemma']])\n",
    "fin_data['Subjectivity'] = fin_data['Lemma'].apply(getSubjectivity) \n",
    "fin_data['Polarity'] = fin_data['Lemma'].apply(getPolarity) \n",
    "fin_data['Analysis'] = fin_data['Polarity'].apply(analysis)\n",
    "tb_counts = fin_data.Analysis.value_counts()\n",
    "i = fin_data[(fin_data.Analysis == 'Neutral')].index\n",
    "sentiment = fin_data.drop(i)\n",
    "sentiment\n",
    "sentiment.set_index('date')\n",
    "sentiment.to_csv('sentiment5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93dd05d0-8520-4667-925a-c380ce3ee386",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('2017.txt')\n",
    "# Define a function to clean the text\n",
    "def clean(text):\n",
    "# Removes all special characters and numericals leaving the alphabets\n",
    "    text = re.sub('[^A-Za-z]+', ' ', str(text))\n",
    "    return text\n",
    "\n",
    "# Cleaning the text in the review column\n",
    "mydata['Cleaned Data'] = mydata['article_content'].apply(clean)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist\n",
    "\n",
    "mydata['POS tagged'] = mydata['Cleaned Data'].apply(token_stop_pos)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew\n",
    "\n",
    "mydata['Lemma'] = mydata['POS tagged'].apply(lemmatize)\n",
    "from textblob import TextBlob\n",
    "# function to calculate subjectivity\n",
    "def getSubjectivity(review):\n",
    "    return TextBlob(review).sentiment.subjectivity\n",
    "    # function to calculate polarity\n",
    "def getPolarity(review):\n",
    "        return TextBlob(review).sentiment.polarity\n",
    "\n",
    "# function to analyze the reviews\n",
    "def analysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "fin_data = pd.DataFrame(mydata[['date', 'article_content', 'Lemma']])\n",
    "fin_data['Subjectivity'] = fin_data['Lemma'].apply(getSubjectivity) \n",
    "fin_data['Polarity'] = fin_data['Lemma'].apply(getPolarity) \n",
    "fin_data['Analysis'] = fin_data['Polarity'].apply(analysis)\n",
    "tb_counts = fin_data.Analysis.value_counts()\n",
    "i = fin_data[(fin_data.Analysis == 'Neutral')].index\n",
    "sentiment = fin_data.drop(i)\n",
    "sentiment\n",
    "sentiment.set_index('date')\n",
    "sentiment.to_csv('sentiment6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ba2046-416a-40e1-aa95-72dabb4885db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('2018.txt')\n",
    "# Define a function to clean the text\n",
    "def clean(text):\n",
    "# Removes all special characters and numericals leaving the alphabets\n",
    "    text = re.sub('[^A-Za-z]+', ' ', str(text))\n",
    "    return text\n",
    "\n",
    "# Cleaning the text in the review column\n",
    "mydata['Cleaned Data'] = mydata['article_content'].apply(clean)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist\n",
    "\n",
    "mydata['POS tagged'] = mydata['Cleaned Data'].apply(token_stop_pos)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew\n",
    "\n",
    "mydata['Lemma'] = mydata['POS tagged'].apply(lemmatize)\n",
    "from textblob import TextBlob\n",
    "# function to calculate subjectivity\n",
    "def getSubjectivity(review):\n",
    "    return TextBlob(review).sentiment.subjectivity\n",
    "    # function to calculate polarity\n",
    "def getPolarity(review):\n",
    "        return TextBlob(review).sentiment.polarity\n",
    "\n",
    "# function to analyze the reviews\n",
    "def analysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "fin_data = pd.DataFrame(mydata[['date', 'article_content', 'Lemma']])\n",
    "fin_data['Subjectivity'] = fin_data['Lemma'].apply(getSubjectivity) \n",
    "fin_data['Polarity'] = fin_data['Lemma'].apply(getPolarity) \n",
    "fin_data['Analysis'] = fin_data['Polarity'].apply(analysis)\n",
    "tb_counts = fin_data.Analysis.value_counts()\n",
    "i = fin_data[(fin_data.Analysis == 'Neutral')].index\n",
    "sentiment = fin_data.drop(i)\n",
    "sentiment\n",
    "sentiment.set_index('date')\n",
    "sentiment.to_csv('sentiment7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44d8bf13-cdfc-496a-a7f6-3bbd66115d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('2019.txt')\n",
    "# Define a function to clean the text\n",
    "def clean(text):\n",
    "# Removes all special characters and numericals leaving the alphabets\n",
    "    text = re.sub('[^A-Za-z]+', ' ', str(text))\n",
    "    return text\n",
    "\n",
    "# Cleaning the text in the review column\n",
    "mydata['Cleaned Data'] = mydata['article_content'].apply(clean)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist\n",
    "\n",
    "mydata['POS tagged'] = mydata['Cleaned Data'].apply(token_stop_pos)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew\n",
    "\n",
    "mydata['Lemma'] = mydata['POS tagged'].apply(lemmatize)\n",
    "from textblob import TextBlob\n",
    "# function to calculate subjectivity\n",
    "def getSubjectivity(review):\n",
    "    return TextBlob(review).sentiment.subjectivity\n",
    "    # function to calculate polarity\n",
    "def getPolarity(review):\n",
    "        return TextBlob(review).sentiment.polarity\n",
    "\n",
    "# function to analyze the reviews\n",
    "def analysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "fin_data = pd.DataFrame(mydata[['date', 'article_content', 'Lemma']])\n",
    "fin_data['Subjectivity'] = fin_data['Lemma'].apply(getSubjectivity) \n",
    "fin_data['Polarity'] = fin_data['Lemma'].apply(getPolarity) \n",
    "fin_data['Analysis'] = fin_data['Polarity'].apply(analysis)\n",
    "tb_counts = fin_data.Analysis.value_counts()\n",
    "i = fin_data[(fin_data.Analysis == 'Neutral')].index\n",
    "sentiment = fin_data.drop(i)\n",
    "sentiment\n",
    "sentiment.set_index('date')\n",
    "sentiment.to_csv('sentiment8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f866fe15-3ecd-4245-98be-0e38a9989c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('2020.txt')\n",
    "# Define a function to clean the text\n",
    "def clean(text):\n",
    "# Removes all special characters and numericals leaving the alphabets\n",
    "    text = re.sub('[^A-Za-z]+', ' ', str(text))\n",
    "    return text\n",
    "\n",
    "# Cleaning the text in the review column\n",
    "mydata['Cleaned Data'] = mydata['article_content'].apply(clean)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist\n",
    "\n",
    "mydata['POS tagged'] = mydata['Cleaned Data'].apply(token_stop_pos)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew\n",
    "\n",
    "mydata['Lemma'] = mydata['POS tagged'].apply(lemmatize)\n",
    "from textblob import TextBlob\n",
    "# function to calculate subjectivity\n",
    "def getSubjectivity(review):\n",
    "    return TextBlob(review).sentiment.subjectivity\n",
    "    # function to calculate polarity\n",
    "def getPolarity(review):\n",
    "        return TextBlob(review).sentiment.polarity\n",
    "\n",
    "# function to analyze the reviews\n",
    "def analysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "fin_data = pd.DataFrame(mydata[['date', 'article_content', 'Lemma']])\n",
    "fin_data['Subjectivity'] = fin_data['Lemma'].apply(getSubjectivity) \n",
    "fin_data['Polarity'] = fin_data['Lemma'].apply(getPolarity) \n",
    "fin_data['Analysis'] = fin_data['Polarity'].apply(analysis)\n",
    "tb_counts = fin_data.Analysis.value_counts()\n",
    "i = fin_data[(fin_data.Analysis == 'Neutral')].index\n",
    "sentiment = fin_data.drop(i)\n",
    "sentiment\n",
    "sentiment.set_index('date')\n",
    "sentiment.to_csv('sentiment9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a894f82-d125-4d18-95a5-65b77662b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('2012.txt')\n",
    "# Define a function to clean the text\n",
    "def clean(text):\n",
    "# Removes all special characters and numericals leaving the alphabets\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Cleaning the text in the review column\n",
    "mydata['Cleaned Data'] = mydata['article_content'].apply(clean)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist\n",
    "\n",
    "mydata['POS tagged'] = mydata['Cleaned Data'].apply(token_stop_pos)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew\n",
    "\n",
    "mydata['Lemma'] = mydata['POS tagged'].apply(lemmatize)\n",
    "from textblob import TextBlob\n",
    "# function to calculate subjectivity\n",
    "def getSubjectivity(review):\n",
    "    return TextBlob(review).sentiment.subjectivity\n",
    "    # function to calculate polarity\n",
    "def getPolarity(review):\n",
    "        return TextBlob(review).sentiment.polarity\n",
    "\n",
    "# function to analyze the reviews\n",
    "def analysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "fin_data = pd.DataFrame(mydata[['date', 'article_content', 'Lemma']])\n",
    "fin_data['Subjectivity'] = fin_data['Lemma'].apply(getSubjectivity) \n",
    "fin_data['Polarity'] = fin_data['Lemma'].apply(getPolarity) \n",
    "fin_data['Analysis'] = fin_data['Polarity'].apply(analysis)\n",
    "tb_counts = fin_data.Analysis.value_counts()\n",
    "i = fin_data[(fin_data.Analysis == 'Neutral')].index\n",
    "sentiment = fin_data.drop(i)\n",
    "sentiment\n",
    "sentiment.set_index('date')\n",
    "sentiment.to_csv('sentiment1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
